---
layout: post
title: How to Estimate GPU Memory Usage for LLMs
categories: Engineering
---

# LLM显存占用预估方法

**GPU计算单元**类似CPU中的核，用来进行数值计算。衡量计算量的单位是FLOP：t_he number of floating-point multiplication-adds_，浮点数先乘后加算一个flop。计算能力越强大，速度越快。衡量计算能力的单位是flops： 每秒能执行的flop数量
```
1*2+3                  1 FLOP
1*2 + 3*4 + 4*5        3 FLOP
```

## 存储指标

```
1Byte = 8 bit
1K = 1024 Byte
1M = 1024 K
1G = 1024 M
1T = 1024 G

10 K = 10*1024 Byte
```
除了K、M，G，T等之外，我们常用的还有KB 、MB，GB，TB 。二者有细微的差别。
```
1Byte = 8 bit
1KB = 1000 Byte
1MB = 1000 KB
1GB = 1000 MB
1TB = 1000 GB

10 KB = 10000 Byte
```
K、M，G，T是以1024为底，而KB 、MB，GB，TB以1000为底。不过一般来说，在估算显存大小的时候，我们不需要严格的区分这二者。

## 以ChatGLM为例

> 显存占用 = 模型参数显存占用 + 优化器显存占用 + 样本显存占用

### 模型参数显存占用

| **量化等级** | FP16 |
| --- | --- |
| **参数量** | 62亿 |

1 FP16 = 2 Byte
6.2B * 2Byte = 12.4GB内存

### 优化器的显存占用

优化器如果是**SGD**

$$W_{t+1} = W_{t} - \alpha\triangledown F(W_{t})$$

可以看出来，除了保存$W$之外还要保存对应的梯度$\triangledown F(W)$，因此显存占用等于参数占用显存的两倍。

优化器如果是**Momentum-SGD**

$$\begin{align}
& v_{t+1} = \rho v_{t} + \triangledown F(W_{t}) \\
& W_{t+1} = W{t} - \alpha v_{t+1}
\end{align}$$

这时候还需要保存动量， 因此显存占用等于参数占用显存的三倍。

优化器如果是**Adam**
动量占用的显存更多，因此显存占用等于参数占用显存的四倍。

### 样本显存占用

每个样本经过Embedding层后都会转换成对应的向量，这些向量也都会存储在显存中。

---
---
**Reference**
- [科普帖：深度学习中GPU和显存分析](https://zhuanlan.zhihu.com/p/31558973)