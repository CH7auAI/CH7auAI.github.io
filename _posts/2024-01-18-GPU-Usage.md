---
layout: post
title: How to Estimate GPU Memory Usage for LLMs
categories: Engineering
---

- [LLM GPU Memory Usage Estimation](#llm-gpu-memory-usage-estimation)
  - [Storage](#storage)
    - [K, M, G Storage Unit](#k-m-g-t-storage-unit)
    - [KB, MB, GB Storage Unit](#kb-mb-gb-storage-unit)
  - [Take ChatGLM as an Example](#take-chatglm-as-an-example)
    - [Model Param Usage](#model-param-usage)
    - [Optimizer Usage](#optimizer-usage)
    - [Data Batch Usage](#data-batch-usage)

# LLM GPU Memory Usage Estimation

FLOPs： **FL**oating-point **O**perations **P**er second
```
1*2+0                          1 FLOP
(1*2 + (3*4 + (4*5+0)))        3 FLOP
```

## Storage

### `K, M, G` Storage Unit

```
1Byte = 8 bit
1K = 1024 Byte
1M = 1024 K
1G = 1024 M

10 K = 10*1024 Byte
```

### `KB, MB, GB` Storage Unit

```
1Byte = 8 bit
1KB = 1000 Byte
1MB = 1000 KB
1GB = 1000 MB
1TB = 1000 GB

10 KB = 10000 Byte
```

## Take ChatGLM as an Example

> Total Memory Usage = Model Param Usage + Optimizer Usage + Data Batch Usage

### Model Param Usage

| **Quantization Level** | FP16 |
| --- | --- |
| **Param Size** | 6.2 B |

1 FP16 = 2 Byte

6.2B * 2Byte = 12.4GB 

### Optimizer Usage

优化器如果是**SGD**

$$W_{t+1} = W_{t} - \alpha\triangledown F(W_{t})$$

可以看出来，除了保存$W$之外还要保存对应的梯度$\triangledown F(W)$，因此显存占用等于参数占用显存的两倍。

优化器如果是**Momentum-SGD**

$$\begin{align}
& v_{t+1} = \rho v_{t} + \triangledown F(W_{t}) \\
& W_{t+1} = W{t} - \alpha v_{t+1}
\end{align}$$

这时候还需要保存动量， 因此显存占用等于参数占用显存的三倍。

优化器如果是**Adam**
动量占用的显存更多，因此显存占用等于参数占用显存的四倍。

### Data Batch Usage

每个样本经过Embedding层后都会转换成对应的向量，这些向量也都会存储在显存中。


---
---
**Reference**
- [知乎：深度学习中GPU和显存分析](https://zhuanlan.zhihu.com/p/31558973)